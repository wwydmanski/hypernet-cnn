{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db453527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'hypernet'\n",
      "/home/z1157095/hypernet-cnn/hypernet\n"
     ]
    }
   ],
   "source": [
    "%cd hypernet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9ef57058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3439415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, Optimizer\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9f81b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "411db77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabular_hypernet as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835a7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cea35661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetUpsampler:\n",
    "    def __init__(self, dataset, desired_len):\n",
    "        self.desired_len = desired_len\n",
    "        self.dataset = dataset\n",
    "        self.real_len = len(dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.desired_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.desired_len:\n",
    "            raise Error\n",
    "            \n",
    "        return self.dataset[idx % self.real_len]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6a556f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(size=(100, 900), masked=False, mask_no=200, mask_size=700, shared_mask=False, batch_size=32, test_batch_size=32):\n",
    "    mods = [transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,)),    #mean and std of MNIST\n",
    "        transforms.Lambda(lambda x: torch.flatten(x))]\n",
    "    mods = transforms.Compose(mods)\n",
    "    \n",
    "    trainset = datasets.MNIST(root='./data/train', train=True, download=True, transform=mods)\n",
    "    testset = datasets.MNIST(root='./data/test', train=False, download=True, transform=mods)\n",
    "    \n",
    "    sup_train_size = size[0]\n",
    "    unsup_train_size = size[1]\n",
    "    \n",
    "    if masked:\n",
    "        trainset = MaskedDataset(trainset, mask_no, mask_size)\n",
    "        testset = MaskedDataset(testset, mask_no, mask_size)\n",
    "        if shared_mask:\n",
    "            testset.masks = trainset.masks\n",
    "    \n",
    "    ## supervised training dataset\n",
    "    indices = torch.arange(sup_train_size)\n",
    "    sup_trainset = data_utils.Subset(trainset, indices)\n",
    "    \n",
    "    # balance superivised dataset\n",
    "    sup_trainset = DatasetUpsampler(sup_trainset, unsup_train_size)\n",
    "    \n",
    "    sup_trainloader = torch.utils.data.DataLoader(sup_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    \n",
    "    ## unsupervised training dataset\n",
    "    indices = torch.arange(unsup_train_size) + sup_train_size\n",
    "    print('max', max(indices))\n",
    "    print('min', min(indices))\n",
    "    print('len', len(indices))\n",
    "    unsup_trainset = data_utils.Subset(trainset, indices)\n",
    "    \n",
    "    unsup_trainloader = torch.utils.data.DataLoader(unsup_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    \n",
    "    ## test labeled dataset\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
    "                                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    return sup_trainloader, unsup_trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eaeede88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tensor(59999)\n",
      "min tensor(100)\n",
      "len 59900\n"
     ]
    }
   ],
   "source": [
    "sup_trainloader1, _, _ = get_dataset(size, batch_size=32, test_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6ba6a696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tensor(59999)\n",
      "min tensor(100)\n",
      "len 59900\n"
     ]
    }
   ],
   "source": [
    "sup_trainloader2, _, _ = get_dataset(size, batch_size=32, test_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "53dbfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = sup_trainloader1.dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ee93141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = sup_trainloader2.dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cb3c6dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(all(torch.eq(ds1[i][0], ds2[i][0])))\n",
    "    print(ds1[i][1] == ds2[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24f405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f453d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataLoaderSemi:\n",
    "    def __init__(self, sup_trainloader, unsup_trainloader):\n",
    "        self.sup_trainloader = sup_trainloader\n",
    "        self.unsup_trainloader = unsup_trainloader\n",
    "        \n",
    "        if sup_trainloader.batch_size != unsup_trainloader.batch_size:\n",
    "            raise Error\n",
    "            \n",
    "        self.batch_size = sup_trainloader.batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        if len(self.sup_trainloader) == len(self.unsup_trainloader):\n",
    "            return len(self.unsup_trainloader)\n",
    "        else:\n",
    "            raise Error\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.sup_trainloader, self.unsup_trainloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4651a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tensor(499)\n",
      "min tensor(50)\n",
      "len 450\n"
     ]
    }
   ],
   "source": [
    "sup_trainloader, unsup_trainloader, testloader = get_dataset((50, 450), batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e88afd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unsup_trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aa6a8b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_trainloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c7fd26a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sup_trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "72997f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "be55d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "for n, (i, j) in enumerate(a):\n",
    "    if n==2: \n",
    "        tmp = i[0]\n",
    "        rr = i[1]\n",
    "    if n==52:\n",
    "        print(torch.all(tmp==i[0]))\n",
    "        print(torch.all(rr==i[1]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cf0dbe07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d2b37fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabSSLCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, beta=0.1, unsup_target_wrapper=torch.nn.functional.softmax):\n",
    "        super(TabSSLCrossEntropyLoss, self).__init__()\n",
    "        \n",
    "        self.y_f1 = torch.nn.CrossEntropyLoss()\n",
    "        self.y_f2 = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.f1_f2 = torch.nn.CrossEntropyLoss()\n",
    "        self.f2_f1 = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.unsup_target_wrapper = unsup_target_wrapper\n",
    "    \n",
    "    def forward(self, sup_input, unsup_input):\n",
    "        sup_outputs1, sup_outputs2, sup_labels = sup_input\n",
    "        unsup_outputs1, unsup_outputs2 = unsup_input\n",
    "        \n",
    "        self.supervised_loss = self.y_f1(sup_outputs1, sup_labels) + self.y_f2(sup_outputs2, sup_labels)\n",
    "        \n",
    "        self.self_supervised_loss = self.f1_f2(unsup_outputs1, self.unsup_target_wrapper(unsup_outputs2, dim=1)) \\\n",
    "                                + self.f2_f1(unsup_outputs2, self.unsup_target_wrapper(unsup_outputs1, dim=1))\n",
    "        \n",
    "        return self.supervised_loss + self.beta * self.self_supervised_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdb1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1ee05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0980c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_semisl(hypernet, optimizer, criterion, loaders, data_size, epochs, masks_no,\n",
    "                    changing_beta=None,\n",
    "                    log_to_comet=True,\n",
    "                    experiment=None,\n",
    "                    tag=\"semi-slow-step-hypernet\", \n",
    "                    device='cuda:0', \n",
    "                    project_name=\"semi-hypernetwork\",\n",
    "                    test_every=5):\n",
    "    \"\"\" Train hypernetwork using 2 masks per iteration, one for x1 (sup & unsup), another for x2 (sup & unsup)\"\"\"\n",
    "    trainloader, testloader = loaders\n",
    "    \n",
    "    if log_to_comet:\n",
    "        if experiment is None:\n",
    "            experiment = Experiment(api_key=os.environ.get(\"COMET_KEY\"), project_name=project_name, display_summary_level=0)\n",
    "        experiment.add_tag(tag)\n",
    "        experiment.log_parameter(\"test_nodes\", hypernet.test_nodes)\n",
    "        experiment.log_parameter(\"mask_size\", hypernet.mask_size)\n",
    "        experiment.log_parameter(\"node_hidden_size\", hypernet.node_hidden_size)\n",
    "        experiment.log_parameter(\"lr\", optimizer.defaults['lr'])\n",
    "        experiment.log_parameter(\"training_size\", sum(data_size))\n",
    "        experiment.log_parameter(\"sup_train_size\", data_size[0])\n",
    "        experiment.log_parameter(\"masks_no\", masks_no)\n",
    "        experiment.log_parameter(\"max_epochs\", epochs)\n",
    "        experiment.log_parameter(\"check_val_every_n_epoch\", test_every)\n",
    "        experiment.log_parameter(\"unsupervised_target_wrapper\", criterion.unsup_target_wrapper.__name__)\n",
    "        experiment.log_parameter(\"train_batch_size\", trainloader.batch_size)\n",
    "        experiment.log_parameter(\"test_batch_size\", testloader.batch_size)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accs = []\n",
    "    mask_idx = 0\n",
    "    \n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            total_loss = 0\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            supervised_train_loss = 0.\n",
    "            unsupervised_train_loss = 0.\n",
    "            train_denom = 0\n",
    "    \n",
    "            hypernet.train()\n",
    "            \n",
    "            if changing_beta:\n",
    "                changing_beta(epoch, criterion)\n",
    "            \n",
    "            for i, (sup_data, unsup_data) in enumerate(trainloader):\n",
    "                    \n",
    "                sup_inputs, sup_labels = sup_data\n",
    "                unsup_inputs, _ = unsup_data    \n",
    "                    \n",
    "                sup_inputs = sup_inputs.to(device)\n",
    "                sup_labels = sup_labels.to(device)\n",
    "                unsup_inputs = unsup_inputs.to(device)\n",
    "                \n",
    "                ## f1\n",
    "                masks1 = []\n",
    "                for i in range(len(sup_inputs)):\n",
    "                    masks1.append(hypernet.test_mask[mask_idx])\n",
    "                masks1 = torch.stack(masks1).to(device)\n",
    "                mask_idx = (mask_idx+1) % len(hypernet.test_mask)\n",
    "                \n",
    "                # supervised\n",
    "                sup_outputs1 = hypernet(sup_inputs, masks1)\n",
    "                \n",
    "                # unsupervised\n",
    "                unsup_outputs1 = hypernet(unsup_inputs, masks1)\n",
    "        \n",
    "                ## f2\n",
    "                masks2 = []\n",
    "                for i in range(len(sup_inputs)):\n",
    "                    masks2.append(hypernet.test_mask[mask_idx])\n",
    "                masks2 = torch.stack(masks2).to(device)\n",
    "                mask_idx = (mask_idx+1) % len(hypernet.test_mask)\n",
    "                \n",
    "                # supervised\n",
    "                sup_outputs2 = hypernet(sup_inputs, masks2)\n",
    "                \n",
    "                # unsupervised\n",
    "                unsup_outputs2 = hypernet(unsup_inputs, masks2)\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss = criterion((sup_outputs1, sup_outputs2, sup_labels), (unsup_outputs1, unsup_outputs2))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "\n",
    "                running_loss += loss.item()\n",
    "                supervised_train_loss += criterion.supervised_loss\n",
    "                unsupervised_train_loss += criterion.self_supervised_loss\n",
    "                train_loss.append(loss.item())\n",
    "                train_denom += 1\n",
    "                        \n",
    "            \n",
    "            \n",
    "            if epoch%test_every==0:\n",
    "                if log_to_comet:\n",
    "                    experiment.log_metric(\"beta_coef\", criterion.beta, step=epoch)\n",
    "                    experiment.log_metric('sup_train_loss', supervised_train_loss/train_denom, step=epoch)\n",
    "                    experiment.log_metric('self_sup_train_loss', unsupervised_train_loss/train_denom, step=epoch)\n",
    "                    experiment.log_metric('train_loss', running_loss/train_denom, step=epoch)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # eval\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                denom = 0\n",
    "\n",
    "                test_criterion = torch.nn.CrossEntropyLoss()\n",
    "                hypernet.eval()\n",
    "\n",
    "                for i, data in enumerate(testloader):\n",
    "                    try:\n",
    "                        images, labels, _ = data\n",
    "                    except ValueError:\n",
    "                        images, labels = data\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    denom += len(labels)\n",
    "\n",
    "                    outputs = hypernet(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total_loss += test_criterion(outputs, labels).item()\n",
    "\n",
    "                test_loss.append(total_loss/denom)\n",
    "                test_accs.append(correct/denom*100)\n",
    "\n",
    "                t.set_postfix(test_acc=correct/denom*100, loss=total_loss/i)\n",
    "                \n",
    "                if log_to_comet:\n",
    "                    experiment.log_metric(\"test_accuracy\", correct/len(testloader.dataset)*100, step=epoch)\n",
    "                    experiment.log_metric(\"test_loss\", test_loss[-1], step=epoch)\n",
    "                    \n",
    "                \n",
    "    experiment.end()\n",
    "    return max(test_accs), test_loss[np.argmax(test_accs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "35fd81c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UXrV5UxyhTK3cyQNG6BDuc4bE'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['COMET_KEY'] = 'UXrV5UxyhTK3cyQNG6BDuc4bE'\n",
    "os.environ.get(\"COMET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b2bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c96ed82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_beta(epoch, criterion):\n",
    "    if epoch == 200:\n",
    "        criterion.beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0ce322f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "\n",
    "mask_size = 100\n",
    "masks_no = 200\n",
    "\n",
    "results = defaultdict(list)\n",
    "size = (100, 59900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68697821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df818cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tensor(59999)\n",
      "min tensor(100)\n",
      "len 59900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/abulenok/semi-hypernetwork/45d3f577b62b49cdac25ec1f436a3fe5\n",
      "\n",
      " 22%|████████████████████████▋                                                                                      | 111/500 [5:00:04<16:30:02, 152.71s/it, loss=7.94, test_acc=74.1]"
     ]
    }
   ],
   "source": [
    "for beta in [0.]:\n",
    "    for f in [torch.argmax]:\n",
    "        criterion = TabSSLCrossEntropyLoss(beta=beta, unsup_target_wrapper=f)\n",
    "\n",
    "        hypernet = hp.Hypernetwork(mask_size=mask_size, node_hidden_size=100, test_nodes=masks_no).cuda()\n",
    "\n",
    "        hypernet = hypernet.train()\n",
    "        optimizer = torch.optim.Adam(hypernet.parameters(), lr=3e-4)\n",
    "\n",
    "        # dataset & loaders\n",
    "        sup_trainloader, unsup_trainloader, testloader = get_dataset(size, batch_size=32, test_batch_size=64)\n",
    "        trainloader = TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)\n",
    "\n",
    "        results[size].append(train_semisl(hypernet,\n",
    "                                          optimizer,\n",
    "                                          criterion,\n",
    "                                          (trainloader, testloader), \n",
    "                                          size,\n",
    "                                          epochs,\n",
    "                                          masks_no,\n",
    "                                          changing_beta=None,\n",
    "                                          log_to_comet=True,\n",
    "                                          tag='semisl_whole_dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaff6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe43457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tensor(58999)\n",
      "min tensor(1000)\n",
      "len 58000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/abulenok/semi-hypernetwork/e91e627550094e10b1d625dae7780a91\n",
      "\n",
      " 23%|▋  | 117/500 [3:53:07<12:41:45, 119.34s/it, loss=0.23, test_acc=93.2]"
     ]
    }
   ],
   "source": [
    "###\n",
    "for beta in [0.5]:\n",
    "    for f in [torch.nn.functional.softmax, torch.argmax]:\n",
    "        criterion = TabSSLCrossEntropyLoss(beta=beta, unsup_target_wrapper=f)\n",
    "\n",
    "        hypernet = hp.Hypernetwork(mask_size=mask_size, node_hidden_size=100, test_nodes=masks_no).cuda()\n",
    "\n",
    "        hypernet = hypernet.train()\n",
    "        optimizer = torch.optim.Adam(hypernet.parameters(), lr=3e-4)\n",
    "\n",
    "        # dataset & loaders\n",
    "        sup_trainloader, unsup_trainloader, testloader = get_dataset(size, batch_size=32, test_batch_size=64)\n",
    "        trainloader = TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)\n",
    "\n",
    "        results[size].append(train_semisl(hypernet,\n",
    "                                          optimizer,\n",
    "                                          criterion,\n",
    "                                          (trainloader, testloader), \n",
    "                                          size,\n",
    "                                          epochs,\n",
    "                                          masks_no,\n",
    "                                          changing_beta=None,\n",
    "                                          log_to_comet=True,\n",
    "                                          tag='semisl_whole_dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb058da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
