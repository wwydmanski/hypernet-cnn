{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ef57058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3439415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, Optimizer\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f81b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "411db77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabular_hypernet as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cea35661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetUpsampler:\n",
    "    def __init__(self, dataset, desired_len):\n",
    "        self.desired_len = desired_len\n",
    "        self.dataset = dataset\n",
    "        self.real_len = len(dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.desired_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.desired_len:\n",
    "            raise Error\n",
    "        return self.dataset[idx % self.real_len]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a556f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(size=60000, masked=False, mask_no=200, mask_size=700, shared_mask=False, batch_size=32, test_batch_size=32):\n",
    "    mods = [transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,)),    #mean and std of MNIST\n",
    "        transforms.Lambda(lambda x: torch.flatten(x))]\n",
    "    mods = transforms.Compose(mods)\n",
    "    \n",
    "    trainset = datasets.MNIST(root='./data/train', train=True, download=True, transform=mods)\n",
    "    testset = datasets.MNIST(root='./data/test', train=False, download=True, transform=mods)\n",
    "    \n",
    "    sup_train_size = size // 10\n",
    "    unsup_train_size = size - sup_train_size\n",
    "    \n",
    "    if masked:\n",
    "        trainset = MaskedDataset(trainset, mask_no, mask_size)\n",
    "        testset = MaskedDataset(testset, mask_no, mask_size)\n",
    "        if shared_mask:\n",
    "            testset.masks = trainset.masks\n",
    "    \n",
    "    ## supervised training dataset\n",
    "    indices = torch.arange(sup_train_size)\n",
    "    sup_trainset = data_utils.Subset(trainset, indices)\n",
    "    \n",
    "    # balance superivised dataset\n",
    "    sup_trainset = DatasetUpsampler(sup_trainset, unsup_train_size)\n",
    "    \n",
    "    sup_trainloader = torch.utils.data.DataLoader(sup_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "    \n",
    "    ## unsupervised training dataset\n",
    "    indices = torch.arange(unsup_train_size) + sup_train_size\n",
    "    unsup_trainset = data_utils.Subset(trainset, indices)\n",
    "    \n",
    "    unsup_trainloader = torch.utils.data.DataLoader(unsup_trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "    \n",
    "    ## test labeled dataset\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
    "                                         shuffle=False, num_workers=1)\n",
    "    \n",
    "    return sup_trainloader, unsup_trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f453d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataLoaderSemi:\n",
    "    def __init__(self, sup_trainloader, unsup_trainloader):\n",
    "        self.sup_trainloader = sup_trainloader\n",
    "        self.unsup_trainloader = unsup_trainloader\n",
    "    \n",
    "#     def __next__(self):\n",
    "#         return self.sup_trainloader.__next__(), self.unsup_trainloader.__next__()\n",
    "    def __len__(self):\n",
    "        if len(self.sup_trainloader) == len(self.unsup_trainloader):\n",
    "            return len(self.unsup_trainloader)\n",
    "        else:\n",
    "            raise Error\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.sup_trainloader, self.unsup_trainloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4651a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_trainloader, unsup_trainloader, testloader = get_dataset(500, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e88afd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unsup_trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7fd26a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sup_trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "711902b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18dd4827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be55d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for n, (i, j) in enumerate(a):\n",
    "    #print('lol', i[0].shape, j[0].shape)\n",
    "    if n==2: \n",
    "        tmp = i[0]\n",
    "        rr = i[1]\n",
    "    if n==52:\n",
    "        print(torch.all(tmp==i[0]))\n",
    "        print(torch.all(rr==i[1]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8761b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tabular_hypernet.modules import InsertableNet\n",
    "from tabular_hypernet.training_utils import get_dataloader, train_model\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "class Hypernetwork(torch.nn.Module):\n",
    "    def __init__(self, inp_size=784, out_size=10, mask_size=20, node_hidden_size=20, layers=[64, 256, 128], test_nodes=100, device='cuda:0'):\n",
    "        super().__init__()\n",
    "        self.target_outsize = out_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.mask_size = mask_size\n",
    "        self.input_size = inp_size\n",
    "        self.node_hidden_size = node_hidden_size\n",
    "        \n",
    "        input_w_size = mask_size*node_hidden_size\n",
    "        input_b_size = node_hidden_size\n",
    "\n",
    "        hidden_w_size = node_hidden_size*out_size\n",
    "        hidden_b_size = out_size\n",
    "            \n",
    "        self.out_size = input_w_size+input_b_size+hidden_w_size+hidden_b_size\n",
    "        \n",
    "        self.input = torch.nn.Linear(inp_size, layers[0])\n",
    "        self.hidden1 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.hidden2 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.out = torch.nn.Linear(layers[2], self.out_size)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        \n",
    "#         self.relu = torch.nn.ReLU()\n",
    "        self.relu = torch.relu\n",
    "        self.template = np.zeros(inp_size)\n",
    "        self.test_nodes = test_nodes\n",
    "        self.test_mask = self._create_mask(test_nodes)\n",
    "        \n",
    "        self._retrained = True\n",
    "        self._test_nets = None\n",
    "        \n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device\n",
    "        self.test_mask = self._create_mask(self.test_nodes)\n",
    "        return self\n",
    "        \n",
    "    def forward(self, data, mask=None):\n",
    "        \"\"\" Get a hypernet prediction. \n",
    "        During training we use a single target network per sample. \n",
    "        During eval, we create a network for each test mask and average their results\n",
    "        \n",
    "        Args:\n",
    "            data - prediction input\n",
    "            mask - either None or a torch.tensor((data.shape[0], data.shape[1])).\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self._retrained = True\n",
    "            if mask is None:\n",
    "                masks = np.array([np.random.choice((len(self.template)), self.mask_size, False) for _ in range(len(data))])\n",
    "                tmp = np.array([self.template.copy() for _ in range(len(data))])\n",
    "                for i, mask in enumerate(masks):\n",
    "                    tmp[i, mask] = 1\n",
    "                mask = torch.from_numpy(tmp).to(torch.float32).to(self.device)\n",
    "\n",
    "            \n",
    "            # If we have a few identical masks in a row\n",
    "            # we only need to calculate target network\n",
    "            # for the first one\n",
    "            recalculate = [True]*len(mask)\n",
    "            for i in range(1, len(mask)):\n",
    "                if torch.equal(mask[i-1], mask[i]):\n",
    "                    recalculate[i] = False\n",
    "                    \n",
    "            weights = self.craft_network(mask)\n",
    "            mask = mask.to(torch.bool)\n",
    "            \n",
    "            res = torch.zeros((len(data), self.target_outsize)).to(self.device)\n",
    "            for i in range(len(data)):\n",
    "                if recalculate[i]:\n",
    "                    nn = InsertableNet(weights[i], self.mask_size, self.target_outsize, layers=[self.node_hidden_size])\n",
    "                masked_data = data[i, mask[i]]\n",
    "                res[i] = nn(masked_data)\n",
    "            return res\n",
    "        else:\n",
    "            if mask is None:\n",
    "                mask = self.test_mask\n",
    "                nets = self._get_test_nets()\n",
    "            else:\n",
    "                nets = self.__craft_nets(mask)\n",
    "            mask = mask.to(torch.bool)\n",
    "\n",
    "            res = torch.zeros((len(data), self.target_outsize)).to(self.device)\n",
    "            for i in range(len(mask)):\n",
    "                nn = nets[i]\n",
    "                masked_data = data[:, mask[i]]\n",
    "                res += nn(masked_data)\n",
    "            res /= self.test_nodes\n",
    "            return res\n",
    "\n",
    "    def _get_test_nets(self):\n",
    "        if self._retrained:\n",
    "            nets = self.__craft_nets(self.test_mask)\n",
    "            self._test_nets = nets\n",
    "            self._retrained = False\n",
    "        return self._test_nets\n",
    "    \n",
    "    def __craft_nets(self, mask):\n",
    "        nets = []\n",
    "        weights = self.craft_network(mask.to(torch.float32))\n",
    "        for i in range(len(mask)):\n",
    "            nn = InsertableNet(weights[i], self.mask_size, self.target_outsize, layers=[self.node_hidden_size])\n",
    "            nets.append(nn)\n",
    "        return nets\n",
    "        \n",
    "    def _create_mask(self, count):\n",
    "        print('count', count)\n",
    "        masks = np.array([np.random.choice((len(self.template)), self.mask_size, False) for _ in range(count)])\n",
    "        print('masks', masks.shape)\n",
    "        tmp = np.array([self.template.copy() for _ in range(count)])\n",
    "        for i, mask in enumerate(masks):\n",
    "            tmp[i, mask] = 1\n",
    "        mask = torch.from_numpy(tmp).to(torch.float32).to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def craft_network(self, mask):\n",
    "        out = self.input(mask)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.hidden1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.hidden2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb4fb311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2b37fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabSSLCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, beta=0.1, unsup_target_wrapper=torch.nn.functional.softmax):\n",
    "        super(TabSSLCrossEntropyLoss, self).__init__()\n",
    "        \n",
    "        self.y_f1 = torch.nn.CrossEntropyLoss()\n",
    "        self.y_f2 = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.f1_f2 = torch.nn.CrossEntropyLoss()\n",
    "        self.f2_f1 = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.beta = beta\n",
    "        self.unsup_target_wrapper = unsup_target_wrapper\n",
    "    \n",
    "    def forward(self, sup_input, unsup_input):\n",
    "        sup_outputs1, sup_outputs2, sup_labels = sup_input\n",
    "        unsup_outputs1, unsup_outputs2 = unsup_input\n",
    "        \n",
    "        self.supervised_loss = self.y_f1(sup_outputs1, sup_labels) + self.y_f2(sup_outputs2, sup_labels)\n",
    "        \n",
    "        self.self_supervised_loss = self.f1_f2(unsup_outputs1, self.unsup_target_wrapper(unsup_outputs2, dim=1)) \\\n",
    "                                + self.f2_f1(unsup_outputs2, self.unsup_target_wrapper(unsup_outputs1, dim=1))\n",
    "        \n",
    "        return self.supervised_loss + self.beta * self.self_supervised_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06e78310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 50\n",
      "masks (50, 700)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hypernetwork(\n",
       "  (input): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (hidden1): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (out): Linear(in_features=128, out_features=71110, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernet = Hypernetwork(mask_size=700, node_hidden_size=100, test_nodes=50).cuda()\n",
    "hypernet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e57c84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernet.test_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8ddfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0980c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_semisl(hypernet, optimizer, criterion, loaders, data_size, epochs, masks_no,\n",
    "                    changing_beta=None,\n",
    "                    log_to_comet=True,\n",
    "                    experiment=None,\n",
    "                    tag=\"semi-slow-step-hypernet\", \n",
    "                    device='cuda:0', \n",
    "                    project_name=\"semi-hypernetwork\",\n",
    "                    test_every=5):\n",
    "    \"\"\" Train hypernetwork using 2 masks per iteration, one for x1 (sup & unsup), another for x2 (sup & unsup)\"\"\"\n",
    "    if log_to_comet:\n",
    "        if experiment is None:\n",
    "            experiment = Experiment(api_key=os.environ.get(\"COMET_KEY\"), project_name=project_name, display_summary_level=0)\n",
    "        experiment.add_tag(tag)\n",
    "        experiment.log_parameter(\"test_nodes\", hypernet.test_nodes)\n",
    "        experiment.log_parameter(\"mask_size\", hypernet.mask_size)\n",
    "        experiment.log_parameter(\"node_hidden_size\", hypernet.node_hidden_size)\n",
    "        experiment.log_parameter(\"lr\", optimizer.defaults['lr'])\n",
    "        experiment.log_parameter(\"training_size\", data_size)\n",
    "        experiment.log_parameter(\"masks_no\", masks_no)\n",
    "        experiment.log_parameter(\"max_epochs\", epochs)\n",
    "        experiment.log_parameter(\"check_val_every_n_epoch\", test_every)\n",
    "        experiment.log_parameter(\"unsupervised_target_wrapper\", criterion.unsup_target_wrapper.__name__)\n",
    "\n",
    "    trainloader, testloader = loaders\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accs = []\n",
    "    mask_idx = 0\n",
    "    \n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            total_loss = 0\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            supervised_train_loss = 0.\n",
    "            unsupervised_train_loss = 0.\n",
    "            train_denom = 0\n",
    "    \n",
    "            hypernet.train()\n",
    "            \n",
    "            if changing_beta:\n",
    "                changing_beta(epoch, criterion)\n",
    "            \n",
    "            for i, (sup_data, unsup_data) in enumerate(trainloader):\n",
    "                    \n",
    "                sup_inputs, sup_labels = sup_data\n",
    "                unsup_inputs, _ = unsup_data    \n",
    "                    \n",
    "                sup_inputs = sup_inputs.to(device)\n",
    "                sup_labels = sup_labels.to(device)\n",
    "                unsup_inputs = unsup_inputs.to(device)\n",
    "                \n",
    "                ## f1\n",
    "                masks1 = []\n",
    "                for i in range(len(sup_inputs)):\n",
    "                    masks1.append(hypernet.test_mask[mask_idx])\n",
    "                masks1 = torch.stack(masks1).to(device)\n",
    "                mask_idx = (mask_idx+1) % len(hypernet.test_mask)\n",
    "                \n",
    "                # supervised\n",
    "                sup_outputs1 = hypernet(sup_inputs, masks1)\n",
    "                \n",
    "                # unsup\n",
    "                unsup_outputs1 = hypernet(unsup_inputs, masks1)\n",
    "        \n",
    "                ## f2\n",
    "                masks2 = []\n",
    "                for i in range(len(sup_inputs)):\n",
    "                    masks2.append(hypernet.test_mask[mask_idx])\n",
    "                masks2 = torch.stack(masks2).to(device)\n",
    "                mask_idx = (mask_idx+1) % len(hypernet.test_mask)\n",
    "                \n",
    "                # supervised\n",
    "                sup_outputs2 = hypernet(sup_inputs, masks2)\n",
    "                \n",
    "                # unsup\n",
    "                unsup_outputs2 = hypernet(unsup_inputs, masks2)\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss = criterion((sup_outputs1, sup_outputs2, sup_labels), (unsup_outputs1, unsup_outputs2))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "\n",
    "                running_loss += loss.item()\n",
    "                supervised_train_loss += criterion.supervised_loss\n",
    "                unsupervised_train_loss += criterion.self_supervised_loss\n",
    "                train_loss.append(loss.item())\n",
    "                train_denom += 1\n",
    "                        \n",
    "            \n",
    "            \n",
    "            if epoch%test_every==0:\n",
    "                if log_to_comet:\n",
    "                    experiment.log_metric(\"beta_coef\", criterion.beta, step=epoch)\n",
    "                    experiment.log_metric('sup_train_loss', supervised_train_loss/train_denom, step=epoch)\n",
    "                    experiment.log_metric('self_sup_train_loss', unsupervised_train_loss/train_denom, step=epoch)\n",
    "                    experiment.log_metric('train_loss', running_loss/train_denom, step=epoch)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # eval\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                denom = 0\n",
    "\n",
    "                test_criterion = torch.nn.CrossEntropyLoss()\n",
    "                hypernet.eval()\n",
    "\n",
    "                for i, data in enumerate(testloader):\n",
    "                    try:\n",
    "                        images, labels, _ = data\n",
    "                    except ValueError:\n",
    "                        images, labels = data\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    denom += len(labels)\n",
    "\n",
    "                    outputs = hypernet(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total_loss += test_criterion(outputs, labels).item()\n",
    "\n",
    "                test_loss.append(total_loss/denom)\n",
    "                test_accs.append(correct/denom*100)\n",
    "\n",
    "                t.set_postfix(test_acc=correct/denom*100, loss=total_loss/i)\n",
    "                \n",
    "                if log_to_comet:\n",
    "                    experiment.log_metric(\"test_accuracy\", correct/len(testloader.dataset)*100, step=epoch)\n",
    "                    experiment.log_metric(\"test_loss\", test_loss[-1], step=epoch)\n",
    "                    \n",
    "                \n",
    "    experiment.end()\n",
    "    return max(test_accs), test_loss[np.argmax(test_accs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35fd81c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UXrV5UxyhTK3cyQNG6BDuc4bE'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['COMET_KEY'] = 'UXrV5UxyhTK3cyQNG6BDuc4bE'\n",
    "os.environ.get(\"COMET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c96ed82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_beta(epoch, criterion):\n",
    "    if epoch == 200:\n",
    "        criterion.beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1dd86bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta_lst=torch.arange(0.1, 1, 0.9/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2d0c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta_lst.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe43457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/abulenok/semi-hypernetwork/f40c3433c66b447aad7abc6e8411d3af\n",
      "\n",
      " 70%|█████████████████████████████████████                | 280/400 [25:05<08:04,  4.04s/it, loss=2.73, test_acc=32.5]"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "\n",
    "\n",
    "mask_size = 100\n",
    "masks_no = 200\n",
    "\n",
    "results = defaultdict(list)\n",
    "size = 1000\n",
    "\n",
    "###\n",
    "for f in [torch.nn.functional.softmax, torch.argmax]:\n",
    "    criterion = TabSSLCrossEntropyLoss(beta=1., unsup_target_wrapper=f)\n",
    "\n",
    "    hypernet = hp.Hypernetwork(mask_size=mask_size, node_hidden_size=100, test_nodes=masks_no).cuda()\n",
    "\n",
    "    hypernet = hypernet.train()\n",
    "    optimizer = torch.optim.Adam(hypernet.parameters(), lr=3e-4)\n",
    "\n",
    "    # dataset & loaders\n",
    "    sup_trainloader, unsup_trainloader, testloader = get_dataset(size)\n",
    "    trainloader = TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)\n",
    "\n",
    "    results[size].append(train_semisl(hypernet, \n",
    "                                      optimizer, \n",
    "                                      criterion, \n",
    "                                      (trainloader, testloader), \n",
    "                                      size,\n",
    "                                      epochs,\n",
    "                                      masks_no,\n",
    "                                      changing_beta=None,\n",
    "                                      log_to_comet=True,\n",
    "                                      tag='semisl_initial_experiment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca4f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a32651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_beta(epoch, criterion):\n",
    "    if epoch == 100:\n",
    "        criterion.beta = 0.3\n",
    "    if epoch == 190:\n",
    "        criterion.beta = 0.5\n",
    "    if epoch == 270:\n",
    "        criterion.beta = 0.7\n",
    "    if epoch == 360:\n",
    "        criterion.beta = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "\n",
    "\n",
    "mask_size = 100\n",
    "masks_no = 200\n",
    "\n",
    "results = defaultdict(list)\n",
    "size = 1000\n",
    "\n",
    "###\n",
    "for f in [torch.nn.functional.softmax, torch.argmax]:\n",
    "    criterion = TabSSLCrossEntropyLoss(beta=0.1, unsup_target_wrapper=f)\n",
    "\n",
    "    hypernet = hp.Hypernetwork(mask_size=mask_size, node_hidden_size=100, test_nodes=masks_no).cuda()\n",
    "\n",
    "    hypernet = hypernet.train()\n",
    "    optimizer = torch.optim.Adam(hypernet.parameters(), lr=3e-4)\n",
    "\n",
    "    # dataset & loaders\n",
    "    sup_trainloader, unsup_trainloader, testloader = get_dataset(size)\n",
    "    trainloader = TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)\n",
    "\n",
    "    results[size].append(train_semisl(hypernet, \n",
    "                                      optimizer, \n",
    "                                      criterion, \n",
    "                                      (trainloader, testloader), \n",
    "                                      size,\n",
    "                                      epochs,\n",
    "                                      masks_no,\n",
    "                                      changing_beta=changing_beta,\n",
    "                                      log_to_comet=True,\n",
    "                                      tag='semisl_initial_experiment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870640b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_processing",
   "language": "python",
   "name": "img_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
