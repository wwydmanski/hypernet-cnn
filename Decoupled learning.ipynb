{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07701bf-edd9-438a-b814-60b15e6b65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b0ed7-c70f-4e5d-879c-edbf3f4cbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, Optimizer\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b5021-779f-47db-b0a5-bcc2f875eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee33e6-407f-46af-b6f3-e8428179705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.hypernet_training import SimpleNetwork, Hypernetwork, get_dataset, train_slow_step, test_model, InsertableNet, SimpleNetwork, train_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a56fc-a54f-47f2-ac62-a289a381f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc704613-586d-46dc-8546-014348b1e0e8",
   "metadata": {},
   "source": [
    "## Prepare feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349dc6cc-0899-4ba9-bca0-b9ac93b29f0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hypernet feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef09e1e-634a-498f-a382-9250fb00d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/wwydmanski/hypernetwork/c7a33ea1f32f45ff8fe8a630a5be4bed\n",
      "\n",
      "100%|███████████████████████████████████████████████████████| 300/300 [03:29<00:00,  1.43it/s, loss=2.74, test_acc=69.2]\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70.36, 2.0875586384716325)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_size = 400\n",
    "masks_no = 8\n",
    "epochs = 300\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "size = 100\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "hypernet = Hypernetwork(mask_size=mask_size, node_hidden_size=100, test_nodes=masks_no, device=DEVICE).to(DEVICE)   \n",
    "hypernet = hypernet.train()\n",
    "optimizer = torch.optim.Adam(hypernet.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "\n",
    "trainloader, testloader = get_dataset(size, True, masks_no, mask_size, shared_mask=True)\n",
    "train_slow_step(hypernet, optimizer, criterion, (trainloader, testloader), size, epochs, masks_no, device=DEVICE, tag=\"decoupled-head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12596dc6-d6d2-454c-8578-6b664d8c9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(hypernet, testloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14023e6-a238-43f9-b8c7-dfb4dc067549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, hypernet, device='cpu'):\n",
    "        nns = []\n",
    "        weights = hypernet.craft_network(hypernet.test_mask)\n",
    "        for w in weights:\n",
    "            nns.append(InsertableNet(w.detach().to(device), hypernet.mask_size, layers=[hypernet.node_hidden_size]).to(device))\n",
    "        self.nns = nns\n",
    "        \n",
    "    def extract(self, data):\n",
    "        embeddings = []\n",
    "        for mask, nn in zip(hypernet.test_mask, self.nns):\n",
    "            masked = data[:, mask.to(torch.bool)]\n",
    "            embeddings.append(F.linear(masked, nn.inp_weights, nn.inp_bias))\n",
    "\n",
    "        embeddings = torch.stack(embeddings, axis=-1).mean(axis=-1)\n",
    "#         embeddings = torch.concat(embeddings, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2ea15a-34fd-4809-a0ff-5034fc73c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "extractor = Extractor(hypernet.to('cuda:1'), 'cuda:1')\n",
    "for inputs, labels, _ in trainloader:\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    extracted = extractor.extract(inputs)\n",
    "    print(extracted.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0e069-6149-4a7a-8fe1-1bbf1798e7a0",
   "metadata": {},
   "source": [
    "### Dense feature extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59458165-4270-4b8b-8097-142ce0a934fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/wwydmanski/hypernetwork/705281a99dd343a3888cca7bf0737343\n",
      "\n",
      "100%|███████████████████████████████████████████████████████| 300/300 [02:14<00:00,  2.22it/s, loss=1.18, test_acc=69.3]\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/wwydmanski/hypernetwork/705281a99dd343a3888cca7bf0737343\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [121]         : (0.0014174256939440966, 2.350419521331787)\n",
      "COMET INFO:     test_accuracy [60] : (28.299999999999997, 69.38)\n",
      "COMET INFO:     test_loss [60]     : (0.986374250971354, 2.2277075877556434)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     check_val_every_n_epoch : 5\n",
      "COMET INFO:     max_epochs              : 300\n",
      "COMET INFO:     training_size           : 100\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (58.54 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69.38, 1.175550303589075)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_size = 400\n",
    "masks_no = 8\n",
    "epochs = 300\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "size = 100\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "network = SimpleNetwork(784).to(DEVICE)\n",
    "network = network.train()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=3e-4)\n",
    "\n",
    "trainloader, testloader = get_dataset(size, batch_size=batch_size, test_batch_size=128)\n",
    "train_regular(network, optimizer, criterion, (trainloader, testloader), size, epochs, device=DEVICE, name=\"decoupled-head-dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5d3147-2616-4584-a1d0-5f8653361749",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'images' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3856713/752903499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/hypernet-cnn/hypernet_training.ipynb\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(hypernet, testloader, device, verbose)\u001b[0m\n\u001b[1;32m    808\u001b[0m      \"text\": [\n\u001b[1;32m    809\u001b[0m       \u001b[0;34m\"Test acc: 66.19, loss: 2.1276189618640475\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m      ]\n\u001b[0m\u001b[1;32m    811\u001b[0m     },\n\u001b[1;32m    812\u001b[0m     {\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'images' referenced before assignment"
     ]
    }
   ],
   "source": [
    "test_model(network, testloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e37a58a0-c75f-44e1-be62-1542208b5c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseExtractor:\n",
    "    def __init__(self, network, device='cpu'):\n",
    "        self.nn = network.to(device)\n",
    "        \n",
    "    def extract(self, data):\n",
    "        return self.nn.inp(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06aadbdb-e84f-4166-a035-11df7d6c72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "extractor = DenseExtractor(network.to('cuda:1'), 'cuda:1')\n",
    "for inputs, labels in trainloader:\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    extracted = extractor.extract(inputs)\n",
    "    print(extracted.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a0e77-5e9a-4201-a92f-a161881696e9",
   "metadata": {},
   "source": [
    "## Train predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d46f41a-dc49-420a-9f85-82f106771c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_extracted(hypernet, optimizer, criterion, loaders, data_size, epochs, masks_no, device='cuda:0'):\n",
    "    experiment = Experiment(api_key=os.environ.get(\"COMET_KEY\"), project_name=\"hypernetwork\", display_summary_level=0)\n",
    "    experiment.add_tag(\"decoupled-head-predictor\")\n",
    "    experiment.log_parameter(\"test_nodes\", hypernet.test_nodes)\n",
    "    experiment.log_parameter(\"mask_size\", hypernet.mask_size)\n",
    "    experiment.log_parameter(\"training_size\", data_size)\n",
    "    experiment.log_parameter(\"input_size\", hypernet.input_size)\n",
    "    experiment.log_parameter(\"masks_no\", masks_no)\n",
    "    experiment.log_parameter(\"max_epochs\", epochs)\n",
    "    experiment.log_parameter(\"check_val_every_n_epoch\", 5)\n",
    "    \n",
    "    trainloader, testloader = loaders\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accs = []\n",
    "    mask_idx = 0\n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            total_loss = 0\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            hypernet.train()\n",
    "            for i, data in enumerate(trainloader):\n",
    "                try:\n",
    "                    inputs, labels, _ = data\n",
    "                except ValueError:\n",
    "                    inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                inputs = extractor.extract(inputs).detach().to(device)\n",
    "                masks = []\n",
    "                for i in range(len(inputs)):\n",
    "                    masks.append(hypernet.test_mask[mask_idx])\n",
    "                masks = torch.stack(masks).to(device)\n",
    "                mask_idx = (mask_idx+1) % len(hypernet.test_mask)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = hypernet(inputs, masks)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                correct += (outputs.argmax(1)==labels).float().sum()\n",
    "                total += outputs.shape[0]\n",
    "                running_loss += loss.item()\n",
    "                train_loss.append(loss.item())\n",
    "                if i>0 and i % 100 == 0:\n",
    "                    total_loss += running_loss/100\n",
    "\n",
    "                    running_loss = 0.0\n",
    "                    correct = 0\n",
    "                    total=0\n",
    "\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            denom = 0\n",
    "\n",
    "            hypernet.eval()\n",
    "            if epoch%5==0:\n",
    "                for i, data in enumerate(testloader):\n",
    "                    try:\n",
    "                        images, labels, _ = data\n",
    "                    except ValueError:\n",
    "                        images, labels = data\n",
    "                    images = images.to(device)\n",
    "                    images = extractor.extract(images).detach().to(device)\n",
    "                    \n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    denom += len(labels)\n",
    "\n",
    "                    outputs = hypernet(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total_loss += criterion(outputs, labels).item()\n",
    "\n",
    "                test_loss.append(total_loss/i)\n",
    "                test_accs.append(correct/denom*100)\n",
    "\n",
    "                t.set_postfix(test_acc=correct/denom*100, loss=total_loss/i)\n",
    "                experiment.log_metric(\"test_accuracy\", correct/len(testloader.dataset)*100, step=epoch)\n",
    "                experiment.log_metric(\"test_loss\", test_loss[-1], step=epoch)\n",
    "\n",
    "    experiment.end()\n",
    "    return max(test_accs), test_loss[np.argmax(test_accs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead7d6a-f69d-4c20-9377-d20d5acf83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/wwydmanski/hypernetwork/c483d245945f4602b0d092261fc31117\n",
      "\n",
      " 44%|███████████████████████▊                              | 440/1000 [05:12<04:28,  2.09it/s, loss=2.53, test_acc=68.4]"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "results = defaultdict(lambda: defaultdict(list))\n",
    "size = 100\n",
    "\n",
    "for mask_size in [15, 20, 25]:\n",
    "    for masks_no in [10, 15, 20, 25, 30]:\n",
    "        for i in range(5):\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            hypernet_pred = Hypernetwork(inp_size=100, mask_size=mask_size, node_hidden_size=20, test_nodes=masks_no, device='cuda:1').to(DEVICE)    \n",
    "            hypernet_pred = hypernet_pred.train()\n",
    "            optimizer = torch.optim.Adam(hypernet_pred.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "\n",
    "            trainloader, testloader = get_dataset(size, True, masks_no, mask_size, shared_mask=True)\n",
    "            print(masks_no)\n",
    "            res = train_extracted(hypernet_pred, optimizer, criterion, (trainloader, testloader), size, epochs, masks_no, device=DEVICE)\n",
    "            results[masks_no][mask_size].append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feb62750-93db-4e8c-b2ba-22cc8452b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy\n",
      "8\n",
      "50    67.483\n",
      "70    67.753\n",
      "dtype: float64\n",
      "50\n",
      "50    64.079\n",
      "70    65.040\n",
      "dtype: float64\n",
      "15\n",
      "50    67.211\n",
      "dtype: float64\n",
      "4\n",
      "50    66.865\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy\")\n",
    "for key in results.keys():\n",
    "    def _pad(x):\n",
    "        res = [subitem[0] for subitem in x]\n",
    "        res += [res[-1]]*(10-len(res))\n",
    "        return res\n",
    "        \n",
    "    test_acc_df = pd.DataFrame({i: _pad(j) for i, j in results[key].items()})\n",
    "    print(key)\n",
    "    print(test_acc_df.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab615880-581b-41b5-af83-5ada387cab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss\n",
      "8\n",
      "50    2.651092\n",
      "70    2.489010\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3856713/1062071919.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_acc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msubitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/img_processing/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/img_processing/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# TODO: can we get rid of the dt64tz special case above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     return arrays_to_mgr(\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     )\n",
      "\u001b[0;32m~/.conda/envs/img_processing/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/img_processing/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "print(\"Test loss\")\n",
    "for key in results.keys():\n",
    "    test_acc_df = pd.DataFrame({i: [subitem[1] for subitem in j] for i, j in results[key].items()})\n",
    "    print(key)\n",
    "    print(test_acc_df.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe68809-271b-4c23-8609-12e25c5d6640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_processing",
   "language": "python",
   "name": "img_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
