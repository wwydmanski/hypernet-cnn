{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfde2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1005bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db453527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z1157095/hypernet-cnn/hypernet\n"
     ]
    }
   ],
   "source": [
    "%cd hypernet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef57058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3439415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, Optimizer\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f81b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411db77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabular_hypernet as hp\n",
    "from tabular_hypernet.semisl import SSLCELossWithThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfbc97da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tabular_hypernet.training_utils.get_dataset(size=60000, masked=False, mask_no=200, mask_size=700, shared_mask=False, batch_size=32, test_batch_size=32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.training_utils.get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35fd81c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UXrV5UxyhTK3cyQNG6BDuc4bE'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['COMET_KEY'] = 'UXrV5UxyhTK3cyQNG6BDuc4bE'\n",
    "os.environ.get(\"COMET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b2bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9fd4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLCEPseudoLabel(torch.nn.Module):\n",
    "    def __init__(self, beta=0.1):\n",
    "        super(SSLCEPseudoLabel, self).__init__()\n",
    "        \n",
    "        self.y_f1 = torch.nn.CrossEntropyLoss()\n",
    "        self.y_f2 = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.f1_f2 = torch.nn.CrossEntropyLoss()\n",
    "        self.f2_f1 = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, sup_input, unsup_input):\n",
    "        sup_outputs1, sup_outputs2, sup_labels = sup_input\n",
    "        unsup_outputs1, unsup_outputs2, unsup_labels = unsup_input\n",
    "        \n",
    "        self.supervised_loss = self.y_f1(sup_outputs1, sup_labels) + self.y_f2(sup_outputs2, sup_labels)\n",
    "        \n",
    "        self.self_supervised_loss = self.f1_f2(unsup_outputs1, unsup_labels) \\\n",
    "                                + self.f2_f1(unsup_outputs2, unsup_labels)\n",
    "        \n",
    "        return self.supervised_loss + self.beta * self.self_supervised_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4dc449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691082e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(t):\n",
    "    idx = torch.randperm(t.shape[0])\n",
    "    return t[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38796f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6c13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_semisl_N(hypernet, optimizer, criterion, loaders, data_size, epochs, masks_no,\n",
    "                    changing_beta=None,\n",
    "                    log_to_comet=True,\n",
    "                    experiment=None,\n",
    "                    shuffled_masks=True,                   \n",
    "                    device='cuda:0', \n",
    "                    tags=[\"semi-slow-step-hypernet\"],\n",
    "                    project_name=\"semi-hypernetwork\",\n",
    "                    test_every=5,\n",
    "                    description=None,\n",
    "                    log_params={}\n",
    "                ):\n",
    "    \"\"\" Train hypernetwork using 2 masks per iteration, one for x1 (sup & unsup), another for x2 (sup & unsup)\"\"\"\n",
    "    trainloader, testloader = loaders\n",
    "    \n",
    "    print('!! log to comet is', log_to_comet, '\\n')\n",
    "    \n",
    "    if log_to_comet:\n",
    "        if experiment is None:\n",
    "            experiment = Experiment(api_key=os.environ.get(\"COMET_KEY\"), project_name=project_name, display_summary_level=0)\n",
    "        experiment.add_tags(tags)\n",
    "        experiment.log_parameter(\"test_nodes\", hypernet.test_nodes)\n",
    "        experiment.log_parameter(\"mask_size\", hypernet.mask_size)\n",
    "#         experiment.log_parameter(\"node_hidden_size\", hypernet.node_hidden_size)\n",
    "        experiment.log_parameter(\"lr\", optimizer.defaults['lr'])\n",
    "        experiment.log_parameter(\"training_size\", sum(data_size))\n",
    "        experiment.log_parameter(\"sup_train_size\", data_size[0])\n",
    "        experiment.log_parameter(\"masks_no\", masks_no)\n",
    "        experiment.log_parameter(\"max_epochs\", epochs)\n",
    "        experiment.log_parameter(\"check_val_every_n_epoch\", test_every)\n",
    "        experiment.log_parameter(\"train_batch_size\", trainloader.batch_size)\n",
    "        experiment.log_parameter(\"test_batch_size\", testloader.batch_size)\n",
    "        \n",
    "        for log_par_k in log_params.keys():\n",
    "            experiment.log_parameter(log_par_k, log_params[log_par_k])\n",
    "\n",
    "        \n",
    "        if description: \n",
    "            experiment.log_text(description)      \n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accs = []\n",
    "    mask_idx = 0\n",
    "    \n",
    "    with trange(epochs) as t:\n",
    "        for epoch in t:\n",
    "            total_loss = 0\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            supervised_train_loss = 0.\n",
    "            unsupervised_train_loss = 0.\n",
    "            train_denom = 0\n",
    "    \n",
    "            hypernet.train()\n",
    "            \n",
    "            if changing_beta:\n",
    "                changing_beta(epoch, criterion)\n",
    "\n",
    "            \n",
    "            for i, (sup_data, unsup_data) in enumerate(trainloader):\n",
    "                    \n",
    "                sup_inputs, sup_labels = sup_data\n",
    "                unsup_inputs, _ = unsup_data    \n",
    "                    \n",
    "                sup_inputs = sup_inputs.to(device)\n",
    "                sup_labels = sup_labels.to(device)\n",
    "                unsup_inputs = unsup_inputs.to(device)\n",
    "                \n",
    "                hypernet.eval()\n",
    "                unsup_outputs = hypernet(unsup_inputs)\n",
    "                _, unsup_predicted = torch.max(unsup_outputs.data, 1)\n",
    "                hypernet.train()\n",
    "                \n",
    "                ## f1\n",
    "                masks1 = []\n",
    "                for _ in range(len(sup_inputs)):\n",
    "                    masks1.append(hypernet.test_mask[mask_idx])\n",
    "                masks1 = torch.stack(masks1).to(device)\n",
    "                mask_idx += 1\n",
    "                \n",
    "                if shuffled_masks:\n",
    "                    if mask_idx >= len(hypernet.test_mask):\n",
    "                        hypernet.test_mask = permute(hypernet.test_mask)\n",
    "                        mask_idx = 0\n",
    "                else:\n",
    "                    mask_idx %= len(hypernet.test_mask)\n",
    "                \n",
    "                # supervised\n",
    "                sup_outputs1 = hypernet(sup_inputs, masks1)\n",
    "                \n",
    "                # unsupervised\n",
    "                unsup_outputs1 = hypernet(unsup_inputs, masks1)\n",
    "        \n",
    "                ## f2\n",
    "                masks2 = []\n",
    "                for _ in range(len(sup_inputs)):\n",
    "                    masks2.append(hypernet.test_mask[mask_idx])\n",
    "                masks2 = torch.stack(masks2).to(device)\n",
    "                mask_idx += 1\n",
    "                \n",
    "                if shuffled_masks:\n",
    "                    if mask_idx >= len(hypernet.test_mask):\n",
    "                        hypernet.test_mask = permute(hypernet.test_mask)\n",
    "                        mask_idx = 0\n",
    "                else:\n",
    "                    mask_idx %= len(hypernet.test_mask)\n",
    "                \n",
    "                # supervised\n",
    "                sup_outputs2 = hypernet(sup_inputs, masks2)\n",
    "                \n",
    "                # unsupervised\n",
    "                unsup_outputs2 = hypernet(unsup_inputs, masks2)\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                loss = criterion(\n",
    "                    (sup_outputs1, sup_outputs2, sup_labels), \n",
    "                    (unsup_outputs1, unsup_outputs2, unsup_predicted)\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "\n",
    "                running_loss += loss.item()\n",
    "                supervised_train_loss += criterion.supervised_loss\n",
    "                unsupervised_train_loss += criterion.self_supervised_loss\n",
    "                train_loss.append(loss.item())\n",
    "                train_denom += 1\n",
    "                        \n",
    "            \n",
    "            \n",
    "            if epoch%test_every==0:\n",
    "                if log_to_comet:\n",
    "                    experiment.log_metric(\"beta_coef\", criterion.beta, step=epoch)\n",
    "                    experiment.log_metric('sup_train_loss', supervised_train_loss/train_denom, step=epoch)\n",
    "                    experiment.log_metric('self_sup_train_loss', unsupervised_train_loss/train_denom, step=epoch)\n",
    "                    experiment.log_metric('train_loss', running_loss/train_denom, step=epoch)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # eval\n",
    "                total_loss = 0\n",
    "                correct = 0\n",
    "                denom = 0\n",
    "\n",
    "                test_criterion = torch.nn.CrossEntropyLoss()\n",
    "                hypernet.eval()\n",
    "\n",
    "                for i, data in enumerate(testloader):\n",
    "                    try:\n",
    "                        images, labels, _ = data\n",
    "                    except ValueError:\n",
    "                        images, labels = data\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    denom += len(labels)\n",
    "\n",
    "                    outputs = hypernet(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total_loss += test_criterion(outputs, labels).item()\n",
    "\n",
    "                test_loss.append(total_loss/denom)\n",
    "                test_accs.append(correct/denom*100)\n",
    "\n",
    "                t.set_postfix(test_acc=correct/denom*100, loss=total_loss/i)\n",
    "                \n",
    "                if log_to_comet:\n",
    "                    experiment.log_metric(\"test_accuracy\", correct/len(testloader.dataset)*100, step=epoch)\n",
    "                    experiment.log_metric(\"test_loss\", test_loss[-1], step=epoch)\n",
    "    \n",
    "    if experiment:\n",
    "        experiment.end()\n",
    "                                 \n",
    "    return max(test_accs), test_loss[np.argmax(test_accs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688384bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982c0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_beta(epoch, criterion):\n",
    "    if epoch == 0:\n",
    "        criterion.beta = 0\n",
    "    elif epoch == 1:\n",
    "        criterion.beta = 0.1\n",
    "    elif epoch == 90:\n",
    "        criterion.beta = 1.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cc754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d3a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8c2df55",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd37710",
   "metadata": {},
   "source": [
    "### Setup for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbc4d3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59940b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa50ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a2b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5162270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = hp.semisl.get_train_test_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ce322f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "masks_no = 100\n",
    "\n",
    "\n",
    "results = defaultdict(list)\n",
    "size = (100, 59900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68697821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "527b9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#68.6   1.78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b132b",
   "metadata": {},
   "source": [
    "### Test pseudolabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e6a4131",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! log to comet is True \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/abulenok/semi-hypernetwork/93479305d81f4e9287301b75f6fb2d11\n",
      "\n",
      "100%|█████████████████████████████████████████████████████████████████| 100/100 [1:55:17<00:00, 69.17s/it, loss=1.78, test_acc=70.1]\n",
      "COMET ERROR: Error sending a notification, make sure you have opted-in for notifications\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "for lr in [3e-5]:\n",
    "\n",
    "    criterion = SSLCEPseudoLabel(beta=0)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # dataset\n",
    "\n",
    "\n",
    "    hypernet = hp.Hypernetwork(\n",
    "        architecture=torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 64), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "        ),\n",
    "        target_architecture=[(mask_size, 100), (100, 10)],\n",
    "        test_nodes=masks_no,\n",
    "    ).cuda()\n",
    "\n",
    "\n",
    "    hypernet = hypernet.train()\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(hypernet.parameters(), lr=lr)\n",
    "\n",
    "    # loaders\n",
    "    sup_trainloader, unsup_trainloader, testloader = hp.semisl.get_dataloaders(dataset=dataset, size=size, batch_size=32, test_batch_size=64)\n",
    "    trainloader = hp.semisl.TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)\n",
    "\n",
    "    results[size].append(\n",
    "                        train_semisl_N(\n",
    "                            hypernet,\n",
    "                            optimizer,\n",
    "                            criterion,\n",
    "                            (trainloader, testloader), \n",
    "                            size,\n",
    "                            epochs,\n",
    "                            masks_no,\n",
    "                            changing_beta=changing_beta,\n",
    "                            log_to_comet=True,\n",
    "                            tags=['pseudolabel_initial'],\n",
    "                            description=\n",
    "                            \"\"\"\n",
    "                            test pseudolabeling. Labels produced in hypernet.eval mode\n",
    "                            \"\"\",\n",
    "                            log_params={'seed': seed, 'pseudolabels': True}\n",
    "                        )\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b490ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fcfe52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f91808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "!! log to comet is True \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/abulenok/semi-hypernetwork/0309012d15074e1a9b2ea3e5a84abdfa\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████| 100/100 [31:47<00:00, 19.08s/it, loss=1.74, test_acc=72.7]\n",
      "COMET ERROR: Error sending a notification, make sure you have opted-in for notifications\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "for lr in [3e-5]:\n",
    "\n",
    "    criterion = SSLCELossWithThreshold(beta=0)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # dataset\n",
    "\n",
    "\n",
    "    hypernet = hp.Hypernetwork(\n",
    "        architecture=torch.nn.Sequential(\n",
    "            torch.nn.Linear(784, 64), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "        ),\n",
    "        target_architecture=[(mask_size, 100), (100, 10)],\n",
    "        test_nodes=masks_no,\n",
    "    ).cuda()\n",
    "\n",
    "\n",
    "    hypernet = hypernet.train()\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(hypernet.parameters(), lr=lr)\n",
    "\n",
    "    # loaders\n",
    "    sup_trainloader, unsup_trainloader, testloader = hp.semisl.get_dataloaders(dataset=dataset, size=size, batch_size=32, test_batch_size=64)\n",
    "    trainloader = hp.semisl.TrainDataLoaderSemi(sup_trainloader, unsup_trainloader)\n",
    "\n",
    "    results[size].append(\n",
    "                        hp.semisl.train_semisl(\n",
    "                            hypernet,\n",
    "                            optimizer,\n",
    "                            criterion,\n",
    "                            (trainloader, testloader), \n",
    "                            size,\n",
    "                            epochs,\n",
    "                            masks_no,\n",
    "                            changing_beta=None,\n",
    "                            log_to_comet=True,\n",
    "                            tags=['pseudolabel_initial'],\n",
    "                            description=\n",
    "                            \"\"\"\n",
    "                            test pseudolabeling. Labels produced in hypernet.eval mode\n",
    "                            \"\"\",\n",
    "                            log_params={'seed': seed, 'pseudolabels': False}\n",
    "                        )\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d57bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3acf86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
