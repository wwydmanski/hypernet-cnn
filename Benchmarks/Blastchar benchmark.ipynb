{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc12a648-b80a-4eca-afc3-79331e4db7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdadf44-450a-4006-ac5a-bed9840409f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3085114d-cf61-4625-97c5-10f34c00d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular_hypernet import Hypernetwork\n",
    "from tabular_hypernet.training_utils import train_slow_step, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71891b02-e21f-4fe4-a54a-8c56610bcb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f26fc6-18c5-48a4-a479-cff173008af1",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35c736f-294c-4eea-b7d3-364079f184b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Blastchar/churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb145598-e52e-4595-8c5b-ad94767e5f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples: 5174 (73.5%)\n",
      "1 samples: 1869 (26.5%)\n"
     ]
    }
   ],
   "source": [
    "def show_fractions(arr):\n",
    "    for cls_ in sorted(pd.unique(arr)):\n",
    "        print(f\"{cls_} samples: {(arr==cls_).sum()} ({(arr==cls_).sum()/len(arr)*100:.1f}%)\")\n",
    "\n",
    "processed_data = data.copy()\n",
    "del processed_data[\"customerID\"]\n",
    "\n",
    "y_label_enc = LabelEncoder()\n",
    "y = y_label_enc.fit_transform(processed_data[\"Churn\"].values)\n",
    "del processed_data[\"Churn\"]\n",
    "\n",
    "show_fractions(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb8b71b-7c57-4a04-8f51-4b8bdd6177ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7043, 19)\n"
     ]
    }
   ],
   "source": [
    "label_encoders = {}\n",
    "\n",
    "for col in processed_data.columns:\n",
    "    if processed_data[col].dtype == 'object':\n",
    "        encoder = LabelEncoder()\n",
    "        processed_data[col] = encoder.fit_transform(processed_data[col].values)\n",
    "        label_encoders[col] = encoder\n",
    "        \n",
    "X = processed_data.values\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ebd8d0-d845-41f0-bf71-7bd6cb0d8b4e",
   "metadata": {},
   "source": [
    "### Split it into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "220962a3-1cfd-4dd1-b030-336bff717634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, data, shuffle: bool=False, samples_no: int=None):\n",
    "        samples = samples_no or len(data[0])\n",
    "        self.indices = np.arange(samples)\n",
    "        self.shuffle = True\n",
    "        if shuffle:\n",
    "            self.indices = np.random.permutation(self.indices)\n",
    "        self.index = 0\n",
    "        self.max_samples = samples\n",
    "        self.data_x = data[0].to(torch.float32)\n",
    "        self.data_y = data[1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = np.random.permutation(self.indices)\n",
    "        while self.index < self.max_samples:\n",
    "            _idx = self.indices[self.index]\n",
    "            yield self.data_x[_idx], self.data_y[_idx]\n",
    "            self.index += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_x.shape[0]\n",
    "    \n",
    "def get_dataloader(X, y, size=None, batch_size=32):\n",
    "    train_dataset = GenericDataset((X, y), size)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=1)\n",
    "    \n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0a827a-0e1f-4f5c-b15d-3354f075021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "0 samples: 3863 (73.1%)\n",
      "1 samples: 1419 (26.9%)\n",
      "Test data:\n",
      "0 samples: 1311 (74.4%)\n",
      "1 samples: 450 (25.6%)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(\"Training data:\")\n",
    "show_fractions(y_train)\n",
    "\n",
    "print(\"Test data:\")\n",
    "show_fractions(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe0004-328f-4646-9572-1c2939fbe7dd",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b13ebb-c3bb-4299-a4c8-60f199b5f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41f75592-2256-445e-b926-6ab9e2b2481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = [torch.from_numpy(x) for x in [X_train, X_test, y_train, y_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223075b-b7d8-414e-97c9-ade8bda5f0fd",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa90ca0-8a00-4491-b111-ce7eee0e9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def _summarize_results(y_pred, y_score, y_test, labels):\n",
    "    results = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        y_pred_filt = y_pred[y_test==idx]\n",
    "        y_test_filt = y_test[y_test==idx]\n",
    "        acc = (y_pred_filt==y_test_filt.numpy()).sum()/len(y_test_filt)*100\n",
    "        results.append({\n",
    "            \"Class\": label,\n",
    "            \"Accuracy\": acc\n",
    "        })\n",
    "        \n",
    "    acc = (y_pred==y_test.numpy()).sum()/len(y_test)*100    \n",
    "    results.append({\n",
    "        \"Class\": \"Total\",\n",
    "        \"Accuracy\": acc\n",
    "    })\n",
    "    results.append({\n",
    "        \"Class\": \"Loss\",\n",
    "        \"Accuracy\": criterion(torch.from_numpy(y_score), y_test).item()\n",
    "    })\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_model(model_fn, train_data, test_data, label_encoder=None, iters=10):\n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "    if label_encoder is not None:\n",
    "        labels = label_encoder.classes_\n",
    "    else:\n",
    "        labels = sorted(pd.unique(test_data))\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for i in trange(iters):\n",
    "        model = model_fn()\n",
    "\n",
    "        model.fit(X_train, y_train);    \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        results.extend(_summarize_results(y_pred, y_score, y_test, labels))\n",
    "\n",
    "    dframe = pd.DataFrame.from_dict(results)\n",
    "    sns.boxplot(data=dframe.iloc[:-1], y=\"Class\", x=\"Accuracy\", orient='h')\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59445b-c0f0-4ee7-bfe9-5c8a57c22b86",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aacc321b-420a-47f3-88af-6bb02aa0ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5cedd88-43b4-465c-b1e1-6a0b620ae73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_dframe = test_model(lambda: XGBClassifier(verbosity=0, use_label_encoder=False), \n",
    "#                         (X_train, y_train), \n",
    "#                         (X_test, y_test),\n",
    "#                         label_encoder=y_label_enc, iters=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f530a0df-271a-495f-b49d-2cffe523c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60779d-0042-49af-8a32-e6d0b59e1589",
   "metadata": {},
   "source": [
    "## Hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e5536d6-cf9d-4a2b-9d7b-184c0ac27a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedNetwork:\n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.optimizer = torch.optim.Adam(network.parameters(), lr=3e-5)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def fit(self, train_data, epochs=200):\n",
    "        train_model(self.network, self.optimizer, self.criterion, train_data, epochs, DEVICE)\n",
    "                    \n",
    "    def predict(self, dataloader):\n",
    "        res = []\n",
    "        for (X, _) in dataloader:\n",
    "            predictions = self.network(X.to(DEVICE)).cpu().detach().numpy()\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            res.append(predictions)\n",
    "        \n",
    "        res = np.concatenate(res)\n",
    "        return res\n",
    "    \n",
    "    def predict_proba(self, dataloader):\n",
    "        res = []\n",
    "        for (X, _) in dataloader:\n",
    "            predictions = self.network(X.to(DEVICE)).cpu().detach().numpy()\n",
    "            res.append(predictions)\n",
    "        \n",
    "        res = np.concatenate(res)\n",
    "        return res\n",
    "    \n",
    "def get_network(inputs, outputs):\n",
    "    return \n",
    "\n",
    "def network_fn(mask_size, masks_no):\n",
    "    def _inner():\n",
    "        network = Hypernetwork(inp_size=X_train.shape[1], \n",
    "                            out_size=y.max().item()+1, \n",
    "                            mask_size=mask_size,\n",
    "                            layers=[128, 128, 128],\n",
    "                            node_hidden_size=100, \n",
    "                            test_nodes=masks_no).to(DEVICE)\n",
    "\n",
    "        network = AdaptedNetwork(network)\n",
    "        return network\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e19014e-1568-483a-bca2-b9fccf9b8730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_hypernet_model(model_fn, trainloader, testloader, label_encoder=None, iters=10):\n",
    "    if label_encoder is not None:\n",
    "        labels = label_encoder.classes_\n",
    "    else:\n",
    "        labels = sorted(pd.unique(test_data))\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for i in trange(iters):\n",
    "        model = model_fn()\n",
    "\n",
    "        model.fit(trainloader);    \n",
    "        y_pred = model.predict(testloader)\n",
    "        y_score = model.predict_proba(testloader)\n",
    "        results.extend(_summarize_results(y_pred, y_score, y_test, labels))\n",
    "\n",
    "    dframe = pd.DataFrame.from_dict(results)\n",
    "    sns.boxplot(data=dframe.iloc[:-1], y=\"Class\", x=\"Accuracy\", orient='h')\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ebb9bb0-d040-45e7-ac94-ccacc10d1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = get_dataloader(X_train, y_train), get_dataloader(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14ceb3-18e7-405e-8bd7-edeb55c84f53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                       | 0/200 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▍                                                                                              | 1/200 [00:04<14:00,  4.22s/it]\u001b[A\n",
      "  1%|▉                                                                                              | 2/200 [00:08<13:40,  4.14s/it]\u001b[A\n",
      "  2%|█▍                                                                                             | 3/200 [00:12<13:36,  4.15s/it]\u001b[A\n",
      "  2%|█▉                                                                                             | 4/200 [00:16<13:41,  4.19s/it]\u001b[A\n",
      "  2%|██▍                                                                                            | 5/200 [00:21<14:35,  4.49s/it]\u001b[A\n",
      "  3%|██▊                                                                                            | 6/200 [00:26<15:10,  4.69s/it]\u001b[A\n",
      "  4%|███▎                                                                                           | 7/200 [00:31<15:11,  4.72s/it]\u001b[A\n",
      "  4%|███▊                                                                                           | 8/200 [00:36<15:13,  4.76s/it]\u001b[A\n",
      "  4%|████▎                                                                                          | 9/200 [00:40<14:48,  4.65s/it]\u001b[A\n",
      "  5%|████▋                                                                                         | 10/200 [00:44<13:52,  4.38s/it]\u001b[A\n",
      "  6%|█████▏                                                                                        | 11/200 [00:49<14:24,  4.57s/it]\u001b[A\n",
      "  6%|█████▋                                                                                        | 12/200 [00:53<13:13,  4.22s/it]\u001b[A\n",
      "  6%|██████                                                                                        | 13/200 [00:57<13:01,  4.18s/it]\u001b[A\n",
      "  7%|██████▌                                                                                       | 14/200 [01:01<13:26,  4.33s/it]\u001b[A\n",
      "  8%|███████                                                                                       | 15/200 [01:07<14:10,  4.60s/it]\u001b[A\n",
      "  8%|███████▌                                                                                      | 16/200 [01:11<14:07,  4.60s/it]\u001b[A\n",
      "  8%|███████▉                                                                                      | 17/200 [01:16<14:18,  4.69s/it]\u001b[A\n",
      "  9%|████████▍                                                                                     | 18/200 [01:20<13:29,  4.45s/it]\u001b[A\n",
      " 10%|████████▉                                                                                     | 19/200 [01:25<13:33,  4.50s/it]\u001b[A\n",
      " 10%|█████████▍                                                                                    | 20/200 [01:29<13:20,  4.45s/it]\u001b[A\n",
      " 10%|█████████▊                                                                                    | 21/200 [01:34<13:40,  4.58s/it]\u001b[A\n",
      " 11%|██████████▎                                                                                   | 22/200 [01:39<14:09,  4.77s/it]\u001b[A\n",
      " 12%|██████████▊                                                                                   | 23/200 [01:44<14:00,  4.75s/it]\u001b[A\n",
      " 12%|███████████▎                                                                                  | 24/200 [01:48<13:29,  4.60s/it]\u001b[A\n",
      " 12%|███████████▊                                                                                  | 25/200 [01:52<13:04,  4.48s/it]\u001b[A\n",
      " 13%|████████████▏                                                                                 | 26/200 [01:57<13:30,  4.66s/it]\u001b[A\n",
      " 14%|████████████▋                                                                                 | 27/200 [02:02<13:50,  4.80s/it]\u001b[A\n",
      " 14%|█████████████▏                                                                                | 28/200 [02:07<13:33,  4.73s/it]\u001b[A\n",
      " 14%|█████████████▋                                                                                | 29/200 [02:12<13:23,  4.70s/it]\u001b[A\n",
      " 15%|██████████████                                                                                | 30/200 [02:17<13:33,  4.78s/it]\u001b[A\n",
      " 16%|██████████████▌                                                                               | 31/200 [02:21<13:22,  4.75s/it]\u001b[A\n",
      " 16%|███████████████                                                                               | 32/200 [02:26<13:37,  4.86s/it]\u001b[A\n",
      " 16%|███████████████▌                                                                              | 33/200 [02:31<13:47,  4.95s/it]\u001b[A\n",
      " 17%|███████████████▉                                                                              | 34/200 [02:36<13:24,  4.85s/it]\u001b[A\n",
      " 18%|████████████████▍                                                                             | 35/200 [02:41<13:44,  5.00s/it]\u001b[A\n",
      " 18%|████████████████▉                                                                             | 36/200 [02:46<13:23,  4.90s/it]\u001b[A\n",
      " 18%|█████████████████▍                                                                            | 37/200 [02:51<13:28,  4.96s/it]\u001b[A\n",
      " 19%|█████████████████▊                                                                            | 38/200 [02:56<13:24,  4.96s/it]\u001b[A\n",
      " 20%|██████████████████▎                                                                           | 39/200 [03:00<12:34,  4.69s/it]\u001b[A\n",
      " 20%|██████████████████▊                                                                           | 40/200 [03:05<12:57,  4.86s/it]\u001b[A\n",
      " 20%|███████████████████▎                                                                          | 41/200 [03:11<13:04,  4.94s/it]\u001b[A\n",
      " 21%|███████████████████▋                                                                          | 42/200 [03:15<12:49,  4.87s/it]\u001b[A\n",
      " 22%|████████████████████▏                                                                         | 43/200 [03:20<12:31,  4.79s/it]\u001b[A\n",
      " 22%|████████████████████▋                                                                         | 44/200 [03:24<11:56,  4.60s/it]\u001b[A\n",
      " 22%|█████████████████████▏                                                                        | 45/200 [03:29<12:19,  4.77s/it]\u001b[A\n",
      " 23%|█████████████████████▌                                                                        | 46/200 [03:34<12:32,  4.88s/it]\u001b[A\n",
      " 24%|██████████████████████                                                                        | 47/200 [03:39<12:36,  4.94s/it]\u001b[A\n",
      " 24%|██████████████████████▌                                                                       | 48/200 [03:44<12:25,  4.91s/it]\u001b[A\n",
      " 24%|███████████████████████                                                                       | 49/200 [03:49<12:11,  4.85s/it]\u001b[A\n",
      " 25%|███████████████████████▌                                                                      | 50/200 [03:53<11:45,  4.70s/it]\u001b[A\n",
      " 26%|███████████████████████▉                                                                      | 51/200 [03:59<12:03,  4.85s/it]\u001b[A\n",
      " 26%|████████████████████████▍                                                                     | 52/200 [04:03<11:35,  4.70s/it]\u001b[A\n",
      " 26%|████████████████████████▉                                                                     | 53/200 [04:08<11:42,  4.78s/it]\u001b[A\n",
      " 27%|█████████████████████████▍                                                                    | 54/200 [04:13<11:58,  4.92s/it]\u001b[A\n",
      " 28%|█████████████████████████▊                                                                    | 55/200 [04:18<12:04,  5.00s/it]\u001b[A\n",
      " 28%|██████████████████████████▎                                                                   | 56/200 [04:23<11:47,  4.91s/it]\u001b[A\n",
      " 28%|██████████████████████████▊                                                                   | 57/200 [04:28<11:48,  4.95s/it]\u001b[A\n",
      " 29%|███████████████████████████▎                                                                  | 58/200 [04:33<11:52,  5.02s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "nn_results = test_hypernet_model(network_fn(15, 20), trainloader, testloader, y_label_enc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28e564-4425-41c2-9e5a-8d74c1bd5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = defaultdict(list)\n",
    "# for i in range(1):\n",
    "#     for size in [100, 500]:\n",
    "#         hypernet = Hypernetwork(inp_size=X.shape[1], \n",
    "#                             out_size=y.max().item()+1, \n",
    "#                             mask_size=15,\n",
    "#                             layers=[32, 64, 32],\n",
    "#                             node_hidden_size=100, \n",
    "#                             test_nodes=10).to(DEVICE)\n",
    "        \n",
    "#         hypernet = hypernet.train()\n",
    "#         optimizer = torch.optim.Adam(hypernet.parameters(), lr=3e-4)\n",
    "\n",
    "#         trainloader, testloader = get_dataloader(X_train, y_train), get_dataloader(X_test, y_test)\n",
    "\n",
    "#         res = train_slow_step(hypernet, optimizer, criterion, \n",
    "#                           (trainloader, testloader), \n",
    "#                           X_train.shape[0], \n",
    "#                           10, \n",
    "#                           10,\n",
    "#                           test_every=10,\n",
    "#                           tag='blastchar-hypernet',\n",
    "#                           device=DEVICE)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8e97c-e0a4-45c7-b5da-8dff056a929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_processing",
   "language": "python",
   "name": "img_processing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
